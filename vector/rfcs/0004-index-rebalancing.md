# RFC 0004: Index Rebalancing

**Status**: Draft

**Authors**:

- [Rohan Desai](https://github.com/agavra)

## Summary

This RFC details the design for OpenData-Vector’s SPANN index maintenance using
a cluster rebalancing protocol based on the LIRE (Lightweight Incremental
Rebalancing) protocol described in https://arxiv.org/pdf/2410.14452.

## Motivation

OpenData Vector supports vector search using a SPANN-style index. Data vectors
are grouped into clusters. Each cluster is represented by a Centroid vector. The
db maintains the Centroid vectors in memory in a HNSW graph, and stores the
associated data vectors on disk in per-Centroid posting lists. TopK search
proceeds by navigating the HNSW graph to find Centroids near the query vector,
and then loads the associated postings to generate a list of candidate data
vectors. The db then ranks the candidates by distance and returns the top K
candidates.

This approach is fine for a static set of data vectors, but falls apart as
vectors are inserted or updated. With a static set of Centroids, the associated
postings will grow unbounded, and/or become increasingly imbalanced. This
reduces query throughput and drives up the average and tail latency required to
achieve good recall, respectively, as queries need to load more data and
evaluate more vectors.

To maintain throughput and latency in the presence of writes, the db needs some
process to maintain centroids with manageably-sized clusters.

There are 2 well-known approaches to this problem. The first approach is to
periodically recompute the set of centroids. This approach has a number of
drawbacks. It’s very expensive as it requires sampling the data vectors and
recomputing centroids. It’s hard to run in the presence of concurrent writes, as
the index needs to be computed while new data is being added. It’s also wasteful
as it recomputes centroids for parts of the data set that may be well-clustered.

We will instead follow the approach described in
https://arxiv.org/pdf/2410.14452, which uses an incremental rebalance protocol
called LIRE that works by trying to drive the size of each Centroid’s posting
list to some target size by splitting clusters when they grow too large, and
merging them when they are too small. This spreads out the work of index
maintenance allowing for consistent write and query performance.

## Goals
- Specify the protocol for maintaining appropriately sized Centroids
- Define configuration for tuning the behaviour of the rebalance protocol
- Define how rebalance can be done safely in the presence of concurrent upserts
  and queries

## Non-Goals
- Detail the approach for handling concurrent rebalance tasks. We will likely
  need to support concurrent rebalance tasks to achieve sufficient rebalance
  throughput. This is deferred for now.
- Detail the approach for supporting vectors replicated across clusters for
  better recall. This complicates rebalancing as it requires some mechanism for
  invalidating old replicas after vector reassignment. We will instead have
  queries evaluate more candidates to achieve good recall, and then reassess
  after benchmarking.
- Define an approach for full index rebuild. It’s possible that the clusters
  generated by the rebalance protocol are not optimal. It might be worth having
  some mechanism for recomputing the index to improve performance.

## High Level Description of LIRE

LIRE aims to do the following:
1. Keep each centroid’s cluster size in number of vectors within some range
  `[minClusterVectors, maxClusterVectors]`.
2. Make a best effort to respect nearest neighbour posting assignment (NPA):
   each data vector V should be assigned to the cluster whose centroid is
   closest to V. This is best-effort because cluster assignment on insert uses
   ANN, and the algorithm explicitly makes the tradeoff to limit the search
   space for reassignment after splitting a cluster.

When new vectors are inserted, the system uses the in-memory centroid index to
find the cluster with the nearest centroid and adds the vector to it.

When a cluster grows past `maxClusterVectors`, LIRE initiates a Split operation
for that cluster. When a cluster shrinks below `minClusterVectors`, LIRE looks
for a nearby cluster whose size will allow merging while respecting
`maxClusterVectors`, and initiates a Merge for the two clusters.

A Split for a cluster with centroid `C` does the following:
1. Use some clustering algorithm (e.g. KMeans) to compute two new centroids
  `C_0` and `C_1`.
2. Assign every vector from `C` to either `C_0` or `C_1`, whichever is nearer.
3. Compute a set of vectors `R` for potential reassignment. This process
   iterates over all the vectors in `C`, and some configurable number of `C`s
   neighbours and uses a cheap heuristic to decide if the vector might need to
   be reassigned. See the appendix for the detailed heuristic.
4. For each vector `V` in `R`:
   1. Let `C_V` be `V`s current assigned centroid.
      1. Search the in-memory centroid index for `V`s target centroid
         `C_V’`. If `C_V’` != `C_V` then schedule reassign `V` to
         `C_V’`. This reassignment can trigger further splits/merges.

A Merge for 2 clusters with centroid `C` and `C’` does the following:
1. Combine the clusters from the smaller of `C` and `C’` into the larger
   cluster. Let’s say `C’` is smaller.
2. For every vector in `C’`, we need to decide if the vector might need to be
   reassigned and then reassign as described in step 4 above.

## Considerations

### Correctness

Correctness is hard to define for vector dbs since queries are not right or
wrong. Rather, correctness is something you measure using a metric called
*Recall*, which measures, for a TopK query with known results `R`, what % of
query results `R’` are present in `R`.

To a large degree, recall is a tunable tradeoff with performance. For example,
you could configure queries to search more clusters to increase the likelihood
of finding close results at the cost of more query work. Similarly you can tune
the size of the neighbourhood that’s searched when evaluating reassignments
after a split.

On the other hand, implementation or synchronization bugs can cause the index to
become permanently degraded. Let’s try to define some correctness properties we
want to aim for to avoid this. At a high level, we should never drop a vector
from the index, and we should make the best effort possible to preserve NPA.
Let's try do describe what this means in practice. Some of these goals are
probably stronger than necessary, but they should give us something correct
without much overhead:
1. Correct execution of LIRE. Obviously we want to implement LIRE correctly to
   preserve NPA and therefore Recall. So splits/merges should be triggered
   appropriately, a split should assign good new centroids, all vectors from
   the source(s) involved should be moved to the target clusters, and the right
   vectors in the configured neighbourhood should be reassigned. This isn’t
   really material to the design.
2. Serialization of inserts with respect to splits/merges: Insert of a given
   vector should take place either before a split/merge, or after the split
   has created the new centroids and split/merge has removed the old one. This
   ensures splitting can’t miss a vector.
3. Synchronization of concurrent splits/merges. Splits/merges can conflict.
   It’s obviously invalid for a given centroid to participate in concurrent
   splits/merges. As another example, a split of 2 centroids near some vector
   `V` can result in suboptimal vector reassignment depending on which split
   wins when deciding reassignment for `V`. Ideally we should avoid such
   conflicts by avoiding concurrent splits in the same neighbourhood. For this
   RFC, we will simply serialize all splits/merges and see how far that gets
   us. To some extent the latter concern is a performance/recall tradeoff, so
   if/when we want to support concurrent splits we can decide how strict to be
   with conflict detection.

Requirements for queries are looser. Splits/merges aren’t atomic as each one is
made up of expensive read-modify-write operations, and some of these leave the
db in a slightly suboptimal intermediate state. The SPFresh implementation
elects to permit querying this suboptimal state and didn’t point out any
significant loss of recall. So the only thing we’ll aim to guarantee here is
that each intermediate operation leaves the db in a coherent state (where all
vectors are reachable) and queries should observe that state with snapshot
isolation - so for example, a query shouldn’t be able to find a centroid in the
HNSW index for which it can’t observe the relevant postings.

### Performance

**Ingest**

Let’s say that we want to maintain a centroid for every 10 data vectors. This is
the recommended ratio from the SPANN paper. Let’s consider what this means about
our throughput requirements for splits. Merges are relatively infrequent, and
are much cheaper as they only require touching the clusters being merged.

Let’s consider a workload ingesting 10K vectors/second. It’s interesting to
consider both a pure insert workload (e.g. when the db is being populated) and a
steady state workload that is pure updates (or 1:1 deletes/inserts) so that the
db size is stable.

In the update case splits are relatively infrequent since every added vector is
1:1 with another vector being removed. The SPFresh authors observed a split for
.4% (4/1000) updates, and a merge for every .1% (but let’s set merge aside for
now). Splits were configured to search a neighbourhood of 64 centroids for
reassignment, resulting in an average of 5094 vectors (their implementation
replicates vectors 8 ways) considered for reassignment, out of which 79 were
actually reassigned (~1.5% reassign rate). If we project this to 10K
updates/second, we’d need to be able to do 40 splits/second and 3160
reassignments/second.

In the pure insert case, we can expect a single split for every 10 inserts. This
projects to 1000 splits/second and 79000 reassignments/second.

The implication for our design is that we need to be able to handle a fairly
large number of splits, and a very large number of reassignments efficiently to
handle a high insert volume. Splits are straightforward as they can be computed
with a single read of the postings. The resulting writes can all be done as part
of regular write batches. Reassignments are trickier. We can’t afford to read
from the db for every reassigned vector. This makes handling boundary vector
replication trickier. We opt to omit that from the design for now.

**Query**

Our intuition for queries is that (1) query throughput is relatively modest
relative to inserts, and (2) users of vector dbs are ok with trading off some
added latency for the scaling benefits we can provide by building on object
store. Turbopuffer is a good reference here - they advertise 1K queries/second
as the limit for namespaces vs 10K vectors ingested/second. And their advertised
latencies are on the order of 10ms for warm dbs and 100s of ms for cold queries
that read directly from object storage.

Fundamentally, the bulk of the latency added by object storage can be mitigated
by caching on local NVME. The db can also gain further latency improvements at
the cost of both overhead and complexity that we will explicitly defer for this
work.
- You can reduce the neighbourhood of search by replicating vectors close to a
  cluster boundary to nearby clusters. But then this makes reassignment more
  complex as you have to be able to invalidate replicas in other clusters.
- You can try to increase the ratio of centroids, but this makes restarts more
  expensive unless you introduce cluster hierarchy.

For this RFC, we will punt these optimizations and instead rely on search
scanning more clusters at the cost of added latency.

Let’s do some napkin math to make sure this is reasonable. Let’s say that each
posting stores 20 768-dimension float32 vectors, with a u64 id for each. So each
posting is ~61600 bytes. An i3.2xlarge NVME can read at ~1.5GBps. So in 10ms we
can read 272 postings. Figure 2 from the SPANN paper suggests this should be
ample to achieve strong recall for most queries (TODO: I’m assuming they
evaluated before all their optimizations - need to verify this).

## Configuration

- `search_neighbourhood`: The minimum number of nearby centroids that will be
  searched for a given query.
- `split_search_neighbourhood`: The number of neighbouring centroids whose
  postings will be considered for reassignment after a split.
- `split_threshold_vectors`: The minimum number of vectors assigned to a
  centroid before the Write Coordinator triggers a spit.
- `merge_threshold_vectors`: The maximum number of vectors assigned to a
  centroid before the Write Coordinator triggers a merge.
- `max_rebalance_tasks`: The maximum number of enqueued rebalance tasks. If
  the queue size grows larger, then the Write Coordinator pauses ingest.

## Architecture

```ascii
                                                                                                                                                
                                                                ┌────────────────────────┐                                                      
                                                                │                        │                                                      
                                                                │                        │                                                      
                                 ┌────────────────────────┐write│     Centroid Index     ◄──────┐                                               
                 ────────────────┤                        ┼─────►                        │      │                        ┌────────────────────┐ 
 vector  ───────►  Ingest Channel│                        │     │                        │     ┌┴─────────────┐          │                    │ 
 upserts         ────────────────┤                        │     └────────────────────────┘     │              │  read    │                    │ 
                                 │    Write Coordinator   │                                    │   Snapshot   ◄──────────┤       Readers      │ 
                 ────────────────┤                        │     ┌────────────────────────┐     │              │          │                    │ 
             ┌─►  Command Channel│                        ├─────►                        │     └┬─────┬───────┘          │                    │ 
             │   ────────────────┤                        │write│                        │      │     │                  │                    │ 
             │                   └────┬───────────────────┘     │        Storage         ◄──────┘     │                  └────────────────────┘ 
             │                        │                         │                        │            │                                         
             │                        │                         │                        │            │                                         
             │                        │                         └────────────────────────┘            │                                         
             │                        │                                                               │                                         
             │                        │Split/Merge Tasks                                              │                                         
             │                        │                                                               │                                         
             │Split/Merge Commands    │                        ┌────────────────────────────┐         │read                                     
             │                        │                        │                            │         │                                         
             │                        │           ─────────────┤                            │         │                                         
             │                        └──────────► Task Channel│                            │◄────────┘                                         
             │                                    ─────────────┤      Imdex Rebalancer      │                                                   
             │                                                 │                            │                                                   
             └─────────────────────────────────────────────────┤                            │                                                   
                                                               └────────────────────────────┘                                                   
```

The main components involved are the Centroid Index, Write Coordinator, and the
new Index Rebalancer, which is responsible for orchestrating LIRE-style
rebalance tasks.

The Centroid Index is the in-memory HNSW index readers to find the relevant
centroids for a query. They then search the centroids’ postings in storage. The
Write Coordinator serializes writes to the db into batches that are then applied
to storage. It also applies mutations to the Centroid Index. For Vector, the
Write Coordinator consumes from both an Ingest Channel for reading vector
upserts/deletes, and a Command Channel, over which it receives commands from the
Index Rebalancer. It also monitors the size of centroid postings, and enqueues
Split and Merge tasks on the Index Rebalancer when they grow too large or too
small, respectively.

The Index Rebalancer is primarily responsible for executing Split and Merge
tasks. It consumes new tasks from an input channel, and then drives each task to
completion synchronously.

## Detailed Design

### Centroid State Machine

The db will maintain the following state machine for centroids. States will be
maintained in-memory and will be reconstructed on restart. We’ll detail the
specifics in a later section:

```rust
enum CentroidState {
   /// The centroid can be searched and inserted into
   ACTIVE,
   /// The centroid is being removed from the head index. New vectors will not be inserted
   /// to it, and it's postings will not be considered during queries.
   DRAINING
}
```

### Rebalance Task Model

Rebalancing will be orchestrated by the Index Rebalancer by running the following
tasks. Each task will iterate through it’s own state machine. The current state of
each task is written to storage by the Write Command (see next section) that causes
the transition.

**Split**

The Split task splits a centroid `C` into centroids `C_0` and `C_1` as described
above in the overview of LIRE. It has the following state machine:

```rust
/// Defines the state machine for a Split task that splits a centroid C(V) into C'(V') and C''(V'')
/// where C(V) denotes a centroid C with vector set V
enum SplitState {
   /// Initializes the new centroids C' with vectors V' (subset of V) and C'' with vectors
   /// V'' (subset of V''), and marks C as DRAINING. This state will never be persisted to
   /// storage as the first write for the split transitions it from SPLIT->SWEEP
   SPLIT,
   /// Collects any vectors in C that were missed in the original split (due to racing inserts)
   /// and adds them to C' or C''. The write for this state transitions SWEEP->REASSIGN
   SWEEP,
   /// Visits all vectors in C and neighbouring centroids and evaluates for reassignment, first
   /// using the cheap LIRE heuristic comparing relative distance to C vs C' and C'', then by
   /// searching in the centroid index. Reassign all vectors requiring reassignment. The write
   /// for this state completes the split and removes the task from storage.
   REASSIGN
}
```

**Merge**

The merge task merges centroids `C_other` into `C` as described in the overview
of LIRE. It has the following state machine:
```rust
/// Defines the state machine for a Merge task that merges centroids C'(V') into C(V)
enum MergeState {
   /// Mark C' as DRAINING and add V' to V. This state will never be persisted to storage
   /// as the first write for the merge transitions it from MERGE->SWEEP
   MERGE,
   /// Collects any vectors in C' that were missed in the original merge (due to racing inserts)
   /// and adds them to V. The write for this state transitions SWEEP->REASSIGN
   SWEEP,
   /// Visit all vectors in C' and evaluate for reassignment using the centroid index. Reassign
   /// all vectors requiring reassignment. The write for this state completes the split and
   /// removes the task from storage.
   REASSIGN
}
```

#### Storage Format

Rebalance Tasks will be stored in SlateDB with the following format.

**Key Layout:**

```
┌─────────┬─────────────┐─────────────────┐
│ version │ record_tag  │     task_id     │
│ 1 byte  │   1 byte    │ 16 bytes (uuid) │
└─────────┴─────────────┘─────────────────┘
```

- `version` (u8): Key format version (currently `0x01`)
- `record_tag` (u8): Record type `0x09` in high nibble, reserved `0x0` in low nibble
- `task_id` (uuid): The task ID

**Value Schema:**

```
┌────────────────────────────────────────────────────────────────┐
│                     RebalanceTaskValue                         │
├────────────────────────────────────────────────────────────────┤
│  type:    u8 (0=Split,1=Merge)                                 │
│  task:    SplitTask|MergeTask                                  │
│                                                                │
│  SplitTask                                                     │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  c:       u64                                            │  │
│  │  c_0:     u64                                            │  │
│  │  c_1:     u64                                            │  │
│  │  state:   u8 (0=SWEEP, 1=REASSIGN)                       │  │
│  └──────────────────────────────────────────────────────────┘  │
│  MergeTask                                                     │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  c:       u64                                            │  │
│  │  c_other: u64                                            │  │
│  │  state:   u8 (0=SWEEP, 1=REASSIGN)                       │  │
│  └──────────────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────────────┘
```

### Write Commands

The task/centroid state machines are churned by the following write commands
issued by the Index Rebalancer to the Write Coordinator:

```rust
/// Write command sent from the index rebalancer to the write coordinator.
enum WriteCommand {
    Split(SplitCommand),
    SplitSweep(SplitSweepCommand),
    SplitReassign(SplitReassignCommand),
    Merge(MergeCommand),
    MergeSweep(MergeSweepCommand),
    MergeReassign(MergeReassignCommand),
}

type Postings = Vec<(u64, Vec<f32>)>;

struct CentroidPostings {
    c: CentroidEntry,
    vectors: Vec<Postings>,
}

/// Instructs the write coordinator to split a given centroid c into 2 new centroids c0 and c1.
/// After this command is executed, the task is initialized and its state set to SWEEP
struct SplitCommand {
    /// Unique task ID associated with this task
    task_id: u64,
    /// The centroid being split. After processing this command, c will be marked DRAINING
    c: CentroidEntry,
    /// The centroids being created from c. After processing this command, c0 and c1 will be
    /// marked ACTIVE and their postings initialized with the vectors from c specified in
    /// the provided postings.
    c0: CentroidPostings,
    c1: CentroidPostings,
}

/// Sent by the index rebalancer after the SplitCommand is flushed to storage. The index rebalancer
/// scans c's postings for vectors written between computing and applying the original split, and
/// updates c0 and c1 postings with these updates. After this command is applied, the task
/// transitions to REASSIGN
struct SplitSweepCommand {
    /// Unique task ID associated with this task
    task_id: u64,
    /// New postings to be added to c0
    c0: CentroidPostings,
    /// New postings to be added to c1
    c1: CentroidPostings
}

struct VectorReassignment {
    target_centroid_id: u64,
    source_centroid_id: u64,
    vector_id: u64,
}

/// Sent by the index rebalancer after the SplitSweepCommand is applied (does not need to block on
/// flush). After the SplitSweepCommand is applied, the rebalancer computes a list of vectors that
/// need to be reassigned, and specifies them in this command. The write coordinator executes the
/// reassignments by adding a `PostingUpdate` with type `Append` to the target posting list, and
/// a `PostingUpdate` with type `Delete` in the source posting list. After this command is applied,
/// the task is complete and its state is deleted. Centroid c is deleted from the set of centroids.
struct SplitReassignCommand {
    /// Unique task ID associated with this task
    task_id: u64,
    /// The list of vectors that must be reassigned.
    reassignments: Vec<VectorReassignment>
}

/// Instructs the write coordinator to initiate a merge of centroid c_other into centroid c. After
/// this command is applied, the write coordinator transitions c_other to DRAINING and the task
/// is created and its state set to SWEEP.
struct MergeCommand {
    /// Unique task ID associated with this task
    task_id: u64,
    /// The id of the centroid that is the target of the merge.
    c: u64,
    /// The id and postings of the centroid that is to be merged into c.
    c_other: CentroidPostings,
}

/// Sent by the index rebalancer after the MergeCommand is durably flushed (so that the rebalancer
/// can read from the latest snapshot). The index rebalancer scans c_other's postings for vectors
/// written between computing and applying the original merge, and updates c's postings with
/// these updates. After this command is applied, the task transitions to REASSIGN
struct MergeSweepCommand {
    /// Unique task ID associated with this task
    task_id: u64,
    /// The postings to be added to centroid c
    c_other: CentroidPostings,
}

/// Sent by the index rebalancer after the MergeSweepCommand is applied (does not need to block on
/// flush). After the MergeSweepCommand is applied, the index rebalancer computes reassignments
/// for all of c_other's postings that need it, and sends these to the write coordinator to
/// execute. The write coordinator executes the reassignments by adding a `PostingUpdate` with type
/// `Append` to the target posting list, and a `PostingUpdate` with type `Delete` in the source
/// posting list.After this command is applied, the task and c are deleted.
struct MergeReassignCommand {
    /// Unique task ID associated with this task
    task_id: u64,
    reassignments: Vec<VectorReassignment>
}
```

### Rebalance Tasks

Now let’s describe how the Index Rebalancer executes Split and Merge tasks.
Observe that each of these tasks is resumable after a restart by observing the
task’s current state in storage.

**Split Centroid C**

1. Drive the transition to `SWEEP`
   1. Load the current reader Snapshot.
   2. Read `C`s postings.
   3. Compute new centroids `C_0` and `C_1` using KMeans on `C`s postings.
   4. Send a `Split` command to the Write Coordinator with new centroids `C_0`
      and `C_1` and their postings.
   5. Wait for flush
2. Drive the transition to `REASSIGN`
   1. Load the current reader Snapshot
   2. Read `C`s postings.
   3. Assign any vectors missed in (1) to the closer of `C_0` or `C_1`.
   4. Send a `SplitSweep` command to the Write Coordinator to execute the split
      for these vectors. 
3. Finish the Split
   1. Initialize Reassign set to R
   2. For every vector in `C`, use the heuristic to decide if it may need to be
      reassigned. For vectors passing the heuristic, find the target centroid in
      the centroid index. If reassign is required, add to `R`.
   3. Find the nearest `split_search_neighbourhood` centroids of `C` in the
      centroid index. Let this set be `N`
      1. For every centroid `C_neighbour` in `N`
         1. Load `C_neighbour`s postings. For each vector in the postings, use
            the heuristic to decide if it may need to be reassigned. For vectors
            passing the heuristic, find the target centroid in the centroid
            index. If reassign is required, add to `R`.
   1. Send a `SplitReassign` command to the Write Coordinator.

**Merge Centroid C**

1. Drive the transition to `SWEEP`
   1. Load the current reader Snapshot.
   2. Let S be the size of `C`.
   3. Use the Centroid index to find all of `C`s neighbours. Find the closest
      neighbour `C_neighbour` with size `S_neighbour` such that
      `S + S_neighbour < split_threshold_vectors`.
   4. Let `C` be the larger of `C` and `C_neighbour`. Let `C_other` be the
      other centroid.
   5. Send a `Merge` command to the Write Coordinator merging all postings
      from `C_other` into `C`.
2. Drive the transition to `REASSIGN`
   1. Load the current reader Snapshot.
   2. Read `C_other`s postings.
   3. Send a `MergeSweep` command to the Write Coordinator with any vectors
      from (ii) that were not included in (1)
3. Finish the Merge
   1. Initialize Reassign set to R
   2. For all vectors in C_other, search the Centroid index for the nearest
      Centroid. If not C, then add to R.
   3. Send a `MergeReassign` command to the Write Coordinator.

### Concurrency Control

**Inserts**

Split/Merge commands are executed by the Write Coordinator, so they are ordered
serially with respect to inserts. After the initial Split/Merge command, all
newly ingested vectors will be routed to the output centroids of the tasks
(`C_0`/`C_1` for Split, `C` for Merge). Observe that the initial transition for
both tasks is not atomic. It reads from the latest snapshot and then executes
the Split/Merge. So it’s possible that some vectors were inserted to the input
centroids in-between the snapshot and execution of the Split/Merge command. This
is resolved by the corresponding `SplitSweep`/`MergeSweep` command, which pulls
in these vectors. So we’re sure not to violate correctness requirement (2)
above.

**Lire Tasks**

For this RFC we will simply just execute LIRE tasks serially. So the Index
Rebalancer will just consume 1 LIRE task from its queue at a time, and drive it
to completion. We can revisit if our evaluation finds that this is too slow.

If we do want to support concurrent LIRE tasks, we need some way for the Write
Coordinator to detect conflicting tasks and reject the initial `Split/Merge`.
For Merge, this is straightforward - just reject if the involved centroids are
already participating in a merge. For Split, we need to apply the same check. We
may also want to have the Write Coordinator search the
`split_search_neighbourhood` Centroids of a proposed task with each running
task, and reject if there is overlap to avoid reassignment races that can cause
vectors to be assigned to centroids that violate NPA. We’d also need some way
for the Index Rebalancer to re-enqueue rejected tasks.

**Queries**

For queries, we just want to ensure that each query observes a coherent view of
the db, where coherent is defined as a consistent snapshot tied to an epoch
generated by the Write Coordinator.

This requires that (1) the Write Coordinator annotates the current Snapshot with
its epoch, and (2) reads of the in-memory Centroid index are consistent with
that epoch. The mechanism for achieving this is detailed in the next section.

### Centroid Index

As described above, the head index needs some way to filter reads by sequence
number for snapshot consistency. It also needs to allow for centroids to be
deleted after a split or merge.

Both of these constraints can be implemented easily if the index supports native
filtering. Meaning, queries can specify a filter that includes a vector in the
search only if it matches a predicate. We can maintain a `DashMap` that maps
each centroid to a log of its state transitions, with each transition annotated
with a sequence number. This way we can compute each centroid’s state at the
search sequence number when executing a search in the index, and filter
centroid’s in state DRAINING, or those that are deleted.

We’ll also need some mechanism to remove from the index a centroid deleted at
epoch `D` once there are no more snapshots referencing an epoch `E` < `D`. To do
this we’ll maintain a log of deletes with their associated epochs. We’ll also
need to be able to track references to existing snapshots so we know a safe
delete point. This is straightforward to do using the Drop trait.

Finally, the centroid index itself actually needs to support deletes and
filters. The implementation we’re currently using (`hnsw_rs`) does not, so we’ll
switch over to the popular `usearch` library (https://www.unum.cloud/usearch).

### Rebalance Triggers

We will trigger rebalances from the insert path. The Write Coordinator will
maintain the current vector size of each centroid’s postings, updating it as
vectors are inserted, deleted, and reassigned. When a posting grows past
`split_threshold_vectors` and `merge_threshold_vectors`, it will enqueue a Split
or Merge task, respectively, on the Index Rebalancer.

The tricky part is that we need some way to recompute these sizes after a
restart. To make the sizes cheaply recoverable, we’ll track the current size in
storage under a per-centroid key. Deltas to the sizes will be written as merge
rows. Then the Index Rebalancer can probe the stored sizes in the background to
hydrate the Write Coordinator. We could also opt to have queries trigger
splits/merges if they discover overly large postings.

#### Storage Format

We'll store the number of vectors in each centroid using the following format.
The format can be extended to track other stats about centroids.

**Key Layout:**

```
┌─────────┬─────────────┐─────────────────┐
│ version │ record_tag  │  centroid_id    │
│ 1 byte  │   1 byte    │  8 bytes (u64)  │
└─────────┴─────────────┘─────────────────┘
```

- `version` (u8): Key format version (currently `0x01`)
- `record_tag` (u8): Record type `0x0a` in high nibble, reserved `0x0` in low nibble
- `centroid_id` (u64): The centroid ID

**Value Schema:**

```
┌────────────────────────────────────────────────────────────────┐
│                     CentroidStatsValue                         │
├────────────────────────────────────────────────────────────────┤
│  num_vectors:    i32                                           │
└────────────────────────────────────────────────────────────────┘
```

- `num_vectors`: A delta to the number of vectors assigned to the centroid. The
  deltas are summed on merge.

### Backpressure

We need some mechanism to apply backpressure to writes in the event that
rebalancing is not able to keep up with the ingest rate. To handle this, the
Write Coordinator will pause ingest when the number of queued rebalance tasks
grows larger than `max_rebalance_tasks`.

### Crash Recovery

When the db starts, the Index Rebalancer recovers in-flight tasks and rehydrates
posting sizes.

To recover in-flight tasks, it scans all tasks stored in storage, and
re-enqueues them with their current state. The tasks are all resumable as
described above.

To recover posting sizes, it runs a background task to scan current posting list
sizes and sends these to the Write Coordinator with the observed epoch. The
Write Coordinator will need to maintain deltas between the initial snapshot and
add these to the reported sizes before updating its view.

## Monitoring

We’ll track the following stats to monitor rebalance progress.

- `rebalance_tasks_queued`: (gauge) The number of rebalance tasks currently queued.
- `split_count`: (counter) The number of splits executed by the db.
- `merge_count`: (counter) The number of merges executed by the db.
- `reassign_count`: (counter) The number of vectors that have been reassigned.
- `reassign_candidate_count`: (counter) The number of vectors that have been evaluated
  for reassignment.

## Evaluation

TODO: we'll want to do 2 phases of performance evaluation. In the first phase,
we'll do an ablation study with a small dataset (1M vectors) and tune the various
parameters like cluster size and split search neighborhood to find a good set of defaults,
and build some feel about the search neighborhood required for good recall. In the
second phase we'll evaluate against a larger data set (100M-1B) to get representative
numbers.

## Future Work

**N-Way Split**

If rebalancing is significantly lagging behind ingest, a posting list could grow
large enough that it will still be larger than or close to the size threshold
after a split. In this case it would be better to split it N ways with N large
enough to make the posting lists smaller than the threshold.

**Trigger Split on Data Size**

It’s probably worth triggering splits when the actual physical cost of reading a
posting (e.g. its size) becomes too high. This can happen even if posting lists
are short if a posting has lots of vectors moving out of it and SlateDB
compaction has not sufficiently merged the entries. In these cases it may be
worth either triggering a split or just rewriting the posting from the
rebalancer. To do this we’d need to be able to measure the actual data size read
when reading a posting, which would require some extension of the merge operator
and/or SlateDB’s read API to be able to report this back to the user.

**Concurrent LIRE Task Execution**

As described above, we’ve omitted concurrent execution of LIRE tasks from this
RFC. The RFC proposes an approach that we could implement to execute LIRE tasks
concurrently while avoiding conflicts.

**Smarter Backpressure**

The backpressure approach proposed above is rather naive. It would probably be
better to apply backpressure specifically when an insert would cause a given
posting to go over some threshold. The main challenge here is prioritizing the
required split since it’s blocking ingestion.

**Vector Replication**

As described above, we’ve decided to defer implementing boundary replication of
vectors to simplify the implementation for reassignments as part of index
rebalancing. The main challenge is that we need some way to efficiently delete
vectors from replicas.

The naive approach of writing a delete entry for all postings requires storing
each vector’s replicas in storage, and then querying this as part of the LIRE
task. Given the expected volume of splits and associated reassignments, it’s
expected that this read cost will be too high.

Another approach would be to simply revision the vector with a new internal ID
whenever its reassigned. However this will likely blow up the size of the
deletions bitmap. It also doesn’t save the expected cost as we need to read the
forward index to observe the vector’s metadata columns to update inverted index
entries.

Another approach would be to track both an internal id and a generation in
postings (or equivalently, reserve the last 8 bits of the internal id for a
generation) and increment this generation whenever a vector is reassigned. We
can track the current generation for each vector in memory, and then only bump
the internal id when we’ve exhausted the space of generations. Inverted indexes
would always use the internal id without the generation. The main drawback to
this approach is that it becomes difficult to prune postings when compacting, so
it becomes more critical to monitor postings from the writer and force a rewrite
when they accumulate too much garbage.

**Active Recall Measurement**

We should try to actively/continuously measure the recall that the db is
achieving. The main challenge is computing the denominator for recall, which
requires an exhaustive search of the whole db. One idea is to sample a set of N
vectors from the data set, and actively maintain the top K closest neighbours as
data is ingested. We can then measure recall by querying against the vectors in
N. We can periodically rotate the set of sampled vectors by re-initializing the
Top K with an exhaustive search (e.g. hourly or daily).

**Decoupled Ingest from Rebalancing**

The Write Coordinator and Index Rebalancer as proposed here could function as a
separate indexing service that can be viewed as compacting a WAL generated by
some other service. So the system could be decoupled into stateless ingest and
query services that write the WAL and read from SlateDB, respectively, with a
stateful indexing service that maintains the index as described in this RFC.
This would require some reader API that allows for ingesting deltas to be
applied to the in-memory centroid index, and decoupling the WAL from SlateDB. To
make ingest truly stateless we’d also need multi-writer into a WAL. As long as
vector doesn’t need read-modify-write with synchronous acknowledgements, the
existing WAL fence protocol could be extended pretty easily to support this by
having writers retry on the expected WAL ID. The benefit is that ingest and
query become fully stateless so they can be scaled and made HA easily.

**Native Filtering (a la ACORN, Turbopuffer)**

It’s a documented problem that it’s difficult to achieve high recall for queries
that include filters. The naive approach (taken by opendata) is to iterate over
postings and just prune vectors that don’t match the filter. This works, but
causes lots of wasted work as its common for many centroids to not have any
vectors that match the filter. TurboPuffer solves this by tracking the centroids
that contain vectors matching filters in its inverted index, and then pruning
entire centroids during search. However, this is challenging to do in the
presence of rebalancing as reassignments can cause centroids to remove vectors
matching a given filter.

[ACORN](https://arxiv.org/abs/2403.04871) proposes another approach, though I
haven’t gone through it in detail.

## Alternatives Considered

**Recompute Posting Sizes**

We could recompute posting sizes by reading back the full posting. This
basically involves reading back the whole database which will delay rehydrating
sizes and scheduling rebalancing.


**Evaluate Thresholds on Read**

We could evaluate split/merge thresholds on read
rather than write. Though simpler, I opted not to go this route as it delays
splits. For example, the pathological case of backfilling the db (a pure write
workload) wouldn’t do any splits.

## Appendix: Reassignment Filter Heuristic

LIRE uses the following heuristic to determine the vectors that may need to be
reassigned when splitting a centroid `C` into `C_0` and `C_1`.

For every vector `V` assigned to `C_0` or `C_1`, compare the distance from `V`
to `C`. If it is closer to `C` than it’s new centroid, then it’s possible some
neighbour is a better centroid, so add it to `R`.

Find the nearest `N` neighbouring centroids of `C`. `N` is configurable. For
every vector `V` assigned to a neighbor `C_n`, compare the distance to `C` vs
the distance to `C_0` and `C_1`. If either `C_0` or `C_1` are closer, then add
it to `R`. Logically, this means that at least one of the new centroids are
moving closer to `V`, so `V` might need to be reassigned. You might wonder why
not just compare the distance between `V->C_0`/`V->C_1` vs `V->C_n`. This is
because you don’t necessarily know `C_n`. In the authors’ implementation of
SPfresh, each data vector is replicated to multiple neighbouring centroids. So
`C_n` may not be the “true” centroid of `V`. The heuristic just tells you
whether it’s possible that the newly created centroids could be a better
centroid for `V` than its current centroid.

## References

1. **SPANN: Highly-efficient Billion-scale Approximate Nearest Neighbor Search**
   Chen et al., NeurIPS 2021.
   [Paper](https://arxiv.org/abs/2111.08566)
   — Foundational work on disk-based ANN with cluster centroids and posting lists.

2. **SPFresh: Incremental In-Place Update for Billion-Scale Vector Search**
   Zhang et al., SOSP 2023.
   [Paper](https://dl.acm.org/doi/10.1145/3600006.3613166) |
   [PDF](https://www.microsoft.com/en-us/research/wp-content/uploads/2023/08/SPFresh_SOSP.pdf)
   — LIRE rebalancing protocol for maintaining index quality with updates.




